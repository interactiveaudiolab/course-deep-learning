{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b3745d-0dfd-4661-bece-dddb094c98e2",
   "metadata": {
    "id": "7b4f7ae8-9e3c-4543-8c6c-75be5b8bef5d"
   },
   "source": [
    "# Notebook 3: Training a Neural Network for Image Classification\n",
    "\n",
    "In this notebook, we'll train a neural network to classify images rather than two-dimensional synthetic data. We'll also take a look at the components of a typical training pipeline in PyTorch, including datasets, data loaders, checkpointing, and logging.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. [Setup](#setup)  \n",
    "  2. [Data](#data)  \n",
    "     2.1 [Datasets](#datasets)  \n",
    "     2.2 [DataLoaders](#dataloaders)  \n",
    "  3. [A Neural Network for Image Recognition](#nn)  \n",
    "     3.1. [Defining the Network](#definition)  \n",
    "     3.2  [Classification Loss](#loss)  \n",
    "     3.3. [Checkpointing](#checkpoint)  \n",
    "  4. [Putting It All Together: Training Loop](#train)  \n",
    "     4.1 [A Basic Training Loop](#basic)  \n",
    "     4.2 [Data Augmentation](#augmentation)  \n",
    "     4.3 [Logging](#logging)  \n",
    "     4.4 [GPU Acceleration](#gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22747f-31e8-4263-9e74-c499f25416b7",
   "metadata": {
    "id": "9857219d-d2b1-4c38-8c7d-66b5bcfdd6a9",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a0af-7266-4bcd-b87a-2558b15da621",
   "metadata": {
    "id": "lAHCqJevuxAO"
   },
   "source": [
    "Make sure the needed packages are installed and utility code is in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb33fcf-137c-429b-9265-4b402a4ec7d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "# helper code from the course repository\n",
    "!git clone https://github.com/interactiveaudiolab/course-deep-learning.git\n",
    "%cd course-deep-learning/\n",
    "# install common pacakges used for deep learning\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b554b46-be39-4431-8031-b4e41d2d15b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020178c4-31a3-49b8-a2e6-d61e205f9d67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __2.__ <a name=\"data\">Data</a>\n",
    "\n",
    "### __2.1__ <a name=\"datasets\">Datasets</a>\n",
    "\n",
    "In the previous two notebooks, we saw a variety of two-dimensional synthetic datasets. In this notebook, we'll be working with a pre-existing image dataset. Image data is inherently high-dimensional: each pixel corresponds to a single coordinate/dimension (grayscale), or holds three separate coordinates (RGB). For even small images (e.g. 32x32 pixels), this means our inputs can have thousands of dimensions! As a result, image datasets can be fairly large. Additionally, we may need to apply certain __transformations__ or __preprocessing__ steps to our image data before attempting to pass it to a neural network.\n",
    "\n",
    "PyTorch and its corresponding image library, TorchVision, offer a number of utilities to streamline dataset storage, loading, and preprocessing. We'll start by using TorchVision to download the well-known MNIST dataset. This dataset contains 28x28-pixel images of handwritten digits, and our goal will be to predict the correct label given an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431fcf53-5a9b-468b-8287-e873c41d1fbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.MNIST,\n",
       " torch.utils.data.dataset.Subset,\n",
       " torch.utils.data.dataset.Subset)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new directory in which to download the MNIST dataset\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "# download MNIST \"test\" dataset\n",
    "mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "# download MNIST \"train\" dataset and set aside a portion for validation\n",
    "mnist_train_full = datasets.MNIST(data_dir, train=True, download=True)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "type(mnist_test), type(mnist_train), type(mnist_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea4af4-c158-4ef1-a02a-bce0fba736d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Our dataset is now held in three `torch.utils.data.Dataset` objects, each acting as an iterable container from which we can fetch input-label pairs. You should also now see a `data/` directory containing the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932c2a10-d824-4e20-b4c8-1f2cd047c387",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F916C4A7580>, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87582752-ad1f-4c56-a5a2-a9d478a3ec17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F916C4ADAC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mnist_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91827e30-3a0c-4484-8104-e8ab84527b94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efbb56-c23f-43e7-9f99-01a28e376321",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "It appears that our \"test\" dataset contains 10,000 entries, each of which is a tuple holding a `PIL.Image.Image` object and an integer label. Unfortunately, the neural networks we trained in the previous notebook require `torch.Tensor` inputs. We therefore need to apply some preprocessing to these image datasets before we can train a network.\n",
    "\n",
    "TorchVision provides a `Transform` class for building and composing preprocessing stages that can be automatically applied to your image data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1627880-672e-415a-a509-361722bd19e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 7\n",
      "Transformed image shape: torch.Size([1, 28, 28])\n",
      "Transformed image data: 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we'll stack multiple transformations in a single object that will apply them in sequence\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # this is a built-in Transform object to convert images to tensors\n",
    "    lambda x: x>0,  # this is our own transformation function for binarizing MNIST images\n",
    "    lambda x: x.float(),  # this is our own transformation function for converting inputs to floating-point\n",
    "])\n",
    "\n",
    "# grab the first image-label pair from our \"test\" dataset\n",
    "example_img, example_label = mnist_test[0]\n",
    "\n",
    "# apply our sequence of transformations\n",
    "transformed = transform(example_img)\n",
    "print(f\"Image label: {example_label}\")\n",
    "print(\"Transformed image shape:\", transformed.shape)\n",
    "print(f\"Transformed image data: {(', '.join(str(p.item()) for p in transformed.flatten()))[:100]} ...\")\n",
    "\n",
    "# plot our image\n",
    "plt.imshow(transformed.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ee41e-b55e-4e05-a30e-27adb99b7fba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can see that our transform converts MNIST images to floating-point binary vectors -- which we can feed to a neural network! In fact, we can bake our transform directly into our datasets so that it is applied automatically when we go to fetch data. To demonstrate, we'll re-initialize our datasets, this time reading directly from our `data/` folder rather than re-downloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf0ea06-2879-4ab3-80c7-0c5f05bdeaa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# load MNIST \"test\" dataset from disk\n",
    "mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=False, transform=transform)\n",
    "\n",
    "# load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "mnist_train_full = datasets.MNIST(data_dir, train=True, download=False, transform=transform)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "example_img, example_label = mnist_test[0]\n",
    "print(type(example_img), example_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bc892-84ee-4824-81a9-9144d9fe3825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __2.2__ <a name=\"dataloaders\">DataLoaders</a>\n",
    "\n",
    "Given that `torch.utils.data.Dataset` and its subclasses provide an iterable container from which we can fetch input-label pairs, we could go ahead and start traininng a network:\n",
    "\n",
    "```\n",
    "for x, y in myDataset:\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    outputs = myNetwork(x)\n",
    "    \n",
    "    loss = myLoss(outputs, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    ...\n",
    "```\n",
    "\n",
    "However, we often want to load our data in __batches__ while training, typically in a random or __shuffled__ order. PyTorch provides a `DataLoader` class to handle the process of fetching data from a `Dataset` object, including shuffling, custom batch collation, and various random sampling schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f0e984-9e5c-4c1d-9bb5-cd11a7218531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs shape: torch.Size([60, 1, 28, 28]), Batch labels shape: torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "# we'll use a batch size of 60 for training our network\n",
    "batch_size = 60\n",
    "\n",
    "# initialize a DataLoader object for each dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# grab the first batch from one of our DataLoader objects\n",
    "example_batch_img, example_batch_label = next(iter(train_dataloader))\n",
    "\n",
    "# inputs and labels are batched together as tensor objects\n",
    "print(f\"Batch inputs shape: {example_batch_img.shape}, Batch labels shape: {example_batch_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5717ae-f257-4b1c-bcfe-766742a570f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __3.__ <a name=\"nn\">A Neural Network for Image Recognition</a>\n",
    "\n",
    "### __3.1__ <a name=\"definition\">Defining the Network</a>\n",
    "\n",
    "Now that we've seen the data we'll be working with, it's time build a neural network capable of classifying handwritten digits. In the previous notebook, we created a neural network capable of turning two-dimensional inputs into one-dimensional (scalar) predictions. By contrast, our inputs will have 28x28 = 784 dimensions, and our network will have to predict one of ten possible labels (one for each digit 0-9). To accommodate these changes, we'll tweak our network as follows:\n",
    "\n",
    "   1. We'll modify our network's first layer to take 784-dimensional inputs\n",
    "   2. We'll use a larger intermediate layer to allow our network to learn complex decision functions\n",
    "   3. We'll try out the ReLU (rectified linear unit) activation function \n",
    "   4. We'll have our network produce a 10-dimensional vector as output; the index of the largest value in this vector will be our predicted label (e.g. if the first entry has the largest value, our predicted digit will be 0).\n",
    "   \n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"500px\" src=\"https://drive.google.com/uc?export=view&id=1fCIzQT6smKorQAfFmJp7GsGBC9atMo4U\"/>\n",
    "</center>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f316de00-6063-40d8-b048-a3bca6a0789e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [],
   "source": [
    "class MNISTNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # MNIST images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28*28, 1024)\n",
    "        self.layer_2 = torch.nn.Linear(1024, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)  # create an array of flattened images with dimension (batch_size, num_pixels)\n",
    "\n",
    "        # this time, we'll use the ReLU nonlinearity at each layer  \n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.layer_2(x)  # we'll avoid \"squashing\" our final outputs by omitting the sigmoid\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f646c8-1f72-424e-a1c7-d6dd93ad3a00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.2__ <a name=\"loss\">Classification Loss</a>\n",
    "\n",
    "In the previous notebook, we used mean squared error loss to train our neural network. While mean squared error performs well in a number of tasks, it is more common to use __categorical cross-entropy loss__ for multiclass classification. We can think of our network's output as a vector of ten \"class scores,\" one per digit. In training our network, our goal is to make sure that given an input image, the correct class score \"comes out on top.\" We might try to minimize the mean squared error between our network's normalized output and a __one-hot__ vector indexing the correct label\n",
    "\n",
    "```\n",
    "prediction = [0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1, 0.1]\n",
    "target =     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "```\n",
    "\n",
    "However, this objective does not necessarily correspond to our goal of maximizing the score of the target class while keeping all other scores low. Cross entropy loss generally does a better job of capturing this objective for multiclass classification, and its use can be considered equivalent to maximum-likelihood estimation under certain assumptions. We will use PyTorch's implementation, which provides an object capable of both computing the loss on pairs of tensors and computing gradients during the backward pass. We won't go into detail here, but for more info, check out the [official documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60994ee-a190-485a-b245-9c5a8faa3bcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9093)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a PyTorch cross-entropy loss object\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# the loss object takes in a vector of class scores and a vector of target class indices\n",
    "preds = torch.randn(batch_size, 10)  # make a batch of random \"class score\" vectors, each with 10 scores corresponding to digits\n",
    "targets = torch.full((60,), 7).long()  # make a batch of target indices; here, we'll set 7 as the target for all predictions\n",
    "\n",
    "# compute the loss for this batch; by default, CrossEntropyLoss will average over a batch to return a scalar\n",
    "loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033abc3d-7e26-4a19-a05e-276b410584ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __3.3__ <a name=\"checkpoint\">Checkpointing</a>\n",
    "\n",
    "Before we begin training our model, we want to make sure we can save it in some format in case we experience a bug during training or want to use it again later. The process of saving snapshots of a model during training is often called __checkpointing__, and PyTorch offers utilities to make saving and loading models simple. For a neural network, saving a model really means saving its weights (parameters). All PyTorch models have a `.state_dict()` method that exposes their weights as named entries in a dictionary. Using this __state dictionary__ as an interface, we can easily save weights or overwrite them with ones we load from elsewhere. For more info, feel free to check out the [official documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55bb3e5-593f-491d-b804-362342febb3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of network weights: ['layer_1.weight', 'layer_1.bias', 'layer_2.weight', 'layer_2.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "model = MNISTNetwork()\n",
    "print(\"Names of network weights:\", list(model.state_dict().keys()))\n",
    "\n",
    "# save weights to disk\n",
    "torch.save(model.state_dict(), \"dummy_weights.pt\")\n",
    "\n",
    "# load weights from disk and overwrite network weights\n",
    "model.load_state_dict(torch.load(\"dummy_weights.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc6805-b78c-4005-96f8-a192ba040769",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "## __4.__ <a name=\"train\">Putting It All Together: Training Loop</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35115d70-2235-4257-bb3f-524bd0d17551",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __4.1__ <a name=\"basic\">A Basic Loop</a>\n",
    "We're now ready to train a neural network to recognize handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44658ffa-8a06-401e-b8a2-7429c20da354",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val loss 0.469, val acc 0.879, train loss 0.953, train acc 0.794\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 2: val loss 0.370, val acc 0.897, train loss 0.416, train acc 0.886\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 3: val loss 0.329, val acc 0.904, train loss 0.352, train acc 0.899\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 4: val loss 0.307, val acc 0.909, train loss 0.321, train acc 0.907\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 5: val loss 0.290, val acc 0.917, train loss 0.298, train acc 0.914\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 6: val loss 0.276, val acc 0.919, train loss 0.280, train acc 0.918\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 7: val loss 0.262, val acc 0.922, train loss 0.264, train acc 0.923\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 8: val loss 0.252, val acc 0.925, train loss 0.250, train acc 0.927\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 9: val loss 0.239, val acc 0.931, train loss 0.237, train acc 0.931\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Epoch 10: val loss 0.230, val acc 0.934, train loss 0.226, train acc 0.934\n",
      "New best accuracy; saving model weights to mnist_basic.pt\n",
      "Total training time (s): 68.67896604537964\n",
      "test loss 0.222, test acc 0.933\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = MNISTNetwork()\n",
    "\n",
    "# initialize an optimizer to update our model's parameters during training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# prepare to save checkpoints\n",
    "model_path = \"mnist_basic.pt\"\n",
    "\n",
    "# make a new directory in which to download the MNIST dataset\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "# number of iterations through the dataset for training\n",
    "epochs = 10\n",
    "\n",
    "# we'll use a batch size of 60 for training our network\n",
    "batch_size = 60\n",
    "\n",
    "# initialize a Transform object to prepare our data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    lambda x: x>0,\n",
    "    lambda x: x.float(),\n",
    "])\n",
    "\n",
    "# load MNIST \"test\" dataset from disk\n",
    "mnist_test = torchvision.datasets.MNIST(data_dir, train=False, download=False, transform=transform)\n",
    "\n",
    "# load MNIST \"train\" dataset from disk and set aside a portion for validation\n",
    "mnist_train_full = datasets.MNIST(data_dir, train=True, download=False, transform=transform)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train_full, [55000, 5000])\n",
    "\n",
    "# initialize a DataLoader object for each dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "# a PyTorch categorical cross-entropy loss object\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# time training process\n",
    "st = time.time()\n",
    "\n",
    "# time to start training!\n",
    "for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "    \n",
    "    # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # loop through the entire dataset once per epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "        \n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        \n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "        \n",
    "        # compute gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update statistics\n",
    "        train_loss += (loss * len(x))\n",
    "        train_acc += (acc * len(x))\n",
    "        train_total += len(x)\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_acc /= train_total\n",
    "    \n",
    "    # perform validation once per epoch\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # don't compute gradients during validation\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "        \n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "        \n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "            \n",
    "            # update statistics\n",
    "            val_loss += (loss * len(x))\n",
    "            val_acc += (acc * len(x))\n",
    "            val_total += len(x)\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc /= val_total\n",
    "    print(f\"Epoch {epoch_idx + 1}: val loss {val_loss :0.3f}, val acc {val_acc :0.3f}, train loss {train_loss :0.3f}, train acc {train_acc :0.3f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        \n",
    "        best_acc = val_acc\n",
    "        print(f\"New best accuracy; saving model weights to {model_path}\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Total training time (s): {time.time() - st}\")\n",
    "        \n",
    "# load best weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# compute final performance on unseen test data. Typically, this is reserved for after the model development process\n",
    "# in order to provide an unbiased estimate of the model's generalized accuracy\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # don't compute gradients during validation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "\n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "        # update statistics\n",
    "        test_loss += (loss * len(x))\n",
    "        test_acc += (acc * len(x))\n",
    "        test_total += len(x)\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc /= test_total\n",
    "print(f\"test loss {test_loss :0.3f}, test acc {test_acc :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d6aa3-614e-4879-9ce2-a773bd12741f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __4.2__ <a name=\"augmentation\">Data Augmentation</a>\n",
    "\n",
    "We've got a pretty accurate model, but there are plenty of deep learning tricks we can use to squeeze some extra performance. One common practice is __data augmentation__, in which random transformations are applied to inputs during training. This helps in two ways:\n",
    "* Often, datasets are relatively small and imperfectly represent the popluation from which they are sampled. Data augmentation effectively expands the size of the dataset through sampling additional randomized variations of each instance.\n",
    "* We typically want to train a model that is __robust__ against common real-world transformations of its inputs -- that is, a model whose predictions are __invariant__ under these transformations. Data augmentation exposes our model to a chosen set of transformations during training so that it can learn to \"see past\" them.\n",
    "\n",
    "TorchVision provides a number of `Transform` objects designed to perform data augmentation, making it easy to apply transformations automatically when data is fetched from a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46252889-adb1-4ed4-a60c-75ae08064a27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fetch an image from the MNIST dataset\n",
    "example_img, example_label = mnist_train[300]\n",
    "plt.imshow(example_img.squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# perform a random affine transformation of an input (rotation, translation, shear)\n",
    "affine_aug = torchvision.transforms.RandomAffine(degrees=(-30, 30), translate=(0.25, 0.25), shear=(-45, 45))\n",
    "augmented = affine_aug(example_img)\n",
    "plt.imshow(augmented.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743878c-24ce-48e4-b2f1-d83020d23633",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "Because we're effectively increasing the size of the dataset, and due to the computation required to perform each transformation, training with data augmentation may take more time (as measured in both walltime and iterations). It's also worth noting that augmentations are typically applied to the training data only. While we won't go into detail at the moment, feel free to try training with any of the [augmentations offered by TorchVision](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4a93a-2957-4fc8-b79f-a65608214f71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __4.3__ <a name=\"logging\">Logging</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36630388-1ac9-455a-93a6-d8c152650339",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "In the above example, we print running summaries of our model's training performance in order to monitor its progress. This is somewhat clunky and limited - what if we want to plot accuracy in real time, visualize challenging instances, dynamically change what information is displayed, or document and compare across multiple training runs? All these tasks fall under the umbrella of __logging__, and once again, PyTorch provides utilities to simplify the process. We can use PyTorch's built-in TensorBoard support to configure and view training logs without the need for any external database or visualization software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465677aa-2e41-4037-9383-53ad522cc7ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-33b54a9f5e6b797d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-33b54a9f5e6b797d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 5681;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here, we'll initialize TensorBoard. You should see an empty window in this cell, which will populate with\n",
    "# graphs as soon as we run our training code below.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a8cd61-ca42-406b-934d-2c163afa725d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "New best accuracy; saving model weights to mnist_again.pt\n",
      "test loss 0.220, test acc 0.935\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# save all log data to a local directory\n",
    "run_dir = \"logs\"\n",
    "\n",
    "# to clear out TensorBoard and start totally fresh, we'll need to\n",
    "# remove old logs by deleting them from the directory\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# timestamp the logs for each run so we can sort through them \n",
    "run_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "# initialize a SummaryWriter object to handle all logging actions\n",
    "logger = SummaryWriter(log_dir=Path(run_dir) / run_time)\n",
    "\n",
    "# initialize model\n",
    "model = MNISTNetwork()\n",
    "\n",
    "# initialize an optimizer to update our model's parameters during training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# prepare to save checkpoints\n",
    "model_path = \"mnist_again.pt\"\n",
    "\n",
    "# time to start training!\n",
    "for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "    \n",
    "    # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # loop through the entire dataset once per epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "        \n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        \n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "        \n",
    "        # compute gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update statistics\n",
    "        train_loss += (loss * len(x))\n",
    "        train_acc += (acc * len(x))\n",
    "        train_total += len(x)\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_acc /= train_total\n",
    "    \n",
    "    # log progress to TensorBoard\n",
    "    logger.add_scalar(\"train_loss\", train_loss, epoch_idx)\n",
    "    logger.add_scalar(\"train_acc\", train_acc, epoch_idx)\n",
    "    \n",
    "    # perform validation once per epoch\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # don't compute gradients during validation\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "        \n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "        \n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "            \n",
    "            # update statistics\n",
    "            val_loss += (loss * len(x))\n",
    "            val_acc += (acc * len(x))\n",
    "            val_total += len(x)\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc /= val_total\n",
    "    \n",
    "    # log progress to TensorBoard\n",
    "    logger.add_scalar(\"val_loss\", val_loss, epoch_idx)\n",
    "    logger.add_scalar(\"val_acc\", val_acc, epoch_idx)\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        \n",
    "        best_acc = val_acc\n",
    "        print(f\"New best accuracy; saving model weights to {model_path}\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# load best weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# compute final performance on unseen test data. Typically, this is reserved for after the model development process\n",
    "# in order to provide an unbiased estimate of the model's generalized accuracy\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # don't compute gradients during validation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "\n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "        # update statistics\n",
    "        test_loss += (loss * len(x))\n",
    "        test_acc += (acc * len(x))\n",
    "        test_total += len(x)\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc /= test_total\n",
    "print(f\"test loss {test_loss :0.3f}, test acc {test_acc :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7145de-5f89-41c7-a27e-951162c5bb1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "We can also run TensorBoard from the terminal, in which case we can view the logs in a browser by navigating to the correct port on our `localhost`. In the example below, after running the command we would need to point our browser to `localhost:9999`\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir /path/to/logging/directory/ --port 9999\n",
    "```\n",
    "\n",
    "If no port is given, TensorBoard will default to 6006. In fact, the logs from your experiment above should already be visible at `localhost:6006`. TensorBoard will continue serving on this port until the notebook kernel shuts down or you halt the terminal command (e.g. using `ctrl` + `c`), at which point you will not be able to view your logs until you re-start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd262fc-3bf6-4a57-8ad6-7372cda4bded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "source": [
    "### __4.4__ <a name=\"gpu\">GPU Acceleration</a>\n",
    "\n",
    "Neural networks use many operations, such as matrix multiplication, that can be efficiently parallelized and run on modern GPUs (graphics processing units, sometimes called \"video cards\"). As a result, neural network training and inference can see drastic speedups when run on a suitable GPU. PyTorch offers this option for NVIDIA-manufactured GPUs through the [CUDA platform](https://pytorch.org/docs/stable/cuda.html), and provides a simple interface for moving data and computation between the CPU and GPU.\n",
    "\n",
    "In practice, running machine learning code on a GPU may require you to check your device's compatibility and install various drivers; this can be quite a hassle. Luckily, [Google Colab](https://colab.research.google.com/) provides free (albeit limited) access to GPUs in a Jupyter-like notebook environment. If you're already running this code in Colab, you can access a GPU by going to `Runtime` > `Change runtime type`, setting `Hardware accelerator` to `GPU`, and clicking `Save`. Note that this will restart the notebook, meaning you will have to run your code again.\n",
    "\n",
    "Below, we'll try our basic training loop again. This time, however, we'll move our network and data to the GPU, allowing for faster training and inference. While the difference between CPU and GPU may be relatively minor in this case, it can be massive for larger models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "706c9c65-096d-4e30-8e3b-f9dc44b26cd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B06RFVzMuw1S",
    "outputId": "9348bd86-e439-48de-e0a3-f9935787cf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val loss 0.477, val acc 0.882, train loss 0.965, train acc 0.785\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 2: val loss 0.370, val acc 0.894, train loss 0.421, train acc 0.885\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 3: val loss 0.334, val acc 0.905, train loss 0.355, train acc 0.898\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 4: val loss 0.309, val acc 0.910, train loss 0.323, train acc 0.907\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 5: val loss 0.289, val acc 0.918, train loss 0.300, train acc 0.913\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 6: val loss 0.275, val acc 0.922, train loss 0.281, train acc 0.919\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 7: val loss 0.260, val acc 0.926, train loss 0.265, train acc 0.923\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 8: val loss 0.251, val acc 0.928, train loss 0.251, train acc 0.927\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 9: val loss 0.243, val acc 0.930, train loss 0.239, train acc 0.931\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Epoch 10: val loss 0.229, val acc 0.933, train loss 0.228, train acc 0.934\n",
      "New best accuracy; saving model weights to mnist_gpu.pt\n",
      "Total training time (s): 58.50265908241272\n",
      "test loss 0.220, test acc 0.935\n"
     ]
    }
   ],
   "source": [
    "# first, let's check if we can access a compatible GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# initialize model\n",
    "model = MNISTNetwork()\n",
    "\n",
    "# move our model (weights) to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# initialize an optimizer to update our model's parameters during training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# prepare to save checkpoints\n",
    "model_path = \"mnist_gpu.pt\"\n",
    "\n",
    "# time training process\n",
    "st = time.time()\n",
    "\n",
    "# time to start training!\n",
    "for epoch_idx, epoch in enumerate(range(epochs)):\n",
    "    \n",
    "    # keep track of best validation accuracy; if improved upon, save checkpoint\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # loop through the entire dataset once per epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "        \n",
    "        # move data to GPU\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        \n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "        \n",
    "        # compute gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update statistics\n",
    "        train_loss += (loss * len(x))\n",
    "        train_acc += (acc * len(x))\n",
    "        train_total += len(x)\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_acc /= train_total\n",
    "    \n",
    "    # perform validation once per epoch\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # don't compute gradients during validation\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            # unpack data and labels\n",
    "            x, y = batch\n",
    "            \n",
    "            # move data to GPU\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        \n",
    "            # generate predictions and compute loss\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "        \n",
    "            # compute accuracy\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = preds.eq(y).sum().item()/len(y)\n",
    "            \n",
    "            # update statistics\n",
    "            val_loss += (loss * len(x))\n",
    "            val_acc += (acc * len(x))\n",
    "            val_total += len(x)\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc /= val_total\n",
    "    print(f\"Epoch {epoch_idx + 1}: val loss {val_loss :0.3f}, val acc {val_acc :0.3f}, train loss {train_loss :0.3f}, train acc {train_acc :0.3f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        \n",
    "        best_acc = val_acc\n",
    "        print(f\"New best accuracy; saving model weights to {model_path}\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Total training time (s): {time.time() - st}\")\n",
    "        \n",
    "# load best weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# compute final performance on unseen test data. Typically, this is reserved for after the model development process\n",
    "# in order to provide an unbiased estimate of the model's generalized accuracy\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # don't compute gradients during validation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # unpack data and labels\n",
    "        x, y = batch\n",
    "        \n",
    "        # move data to GPU\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # generate predictions and compute loss\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = preds.eq(y).sum().item()/len(y)\n",
    "\n",
    "        # update statistics\n",
    "        test_loss += (loss * len(x))\n",
    "        test_acc += (acc * len(x))\n",
    "        test_total += len(x)\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc /= test_total\n",
    "print(f\"test loss {test_loss :0.3f}, test acc {test_acc :0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs396",
   "language": "python",
   "name": "cs396"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
