{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL Lightning and TensorBoard.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["Exnwbnlyhk-o","3fMHyUaGhj-p"],"toc_visible":true,"authorship_tag":"ABX9TyOkNZDLi9ZC8C2jyL6SwjWI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fXGJ_v2xEsfd"},"source":["# Basics of pytorch_lightning, dataloaders, and tensorboard\n","\n","This is a basic walkthrough of building, training, and using a simple neural network in [PyTorch](https://pytorch.org/) using the [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) package to let us make better-organized code, and displaying information about training in [TensorBoard](https://www.tensorflow.org/tensorboard). We'll load the classic [MNIST dataset](http://yann.lecun.com/exdb/mnist/) from the [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) library of image datasets and feed the network data using a [dataloader](https://pytorch.org/docs/stable/data.html), which is a standard way of handling data for a PyTorch model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Exnwbnlyhk-o"},"source":["## Install and import the needed packages"]},{"cell_type":"markdown","metadata":{"id":"wjbkm66hEngE"},"source":["### Install the needed packages."]},{"cell_type":"code","metadata":{"id":"SDsIUoGDKxNy"},"source":["!pip install torch \n","!pip install torchvision\n","!pip install pytorch_lightning\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAPyUo7jK22C"},"source":["### Now that the packages are installed, import them to this project."]},{"cell_type":"code","metadata":{"id":"9lEdGUS8EjGM"},"source":["# -------------------------------------------------\n","#  Pytorch  \n","# -------------------------------------------------\n","\n","# This is the main torch package\n","import torch \n","#Computer vision specific package              \n","import torchvision\n","#There are a bunch of standard datasets in torchvision. \n","import torchvision.datasets as datasets\n","\n","# -------------------------------------------------\n","# Pytorch Lightning \n","# -------------------------------------------------\n","\n","import pytorch_lightning as pl\n","# this gives us the hooks to connect to TensorBoard\n","import pytorch_lightning.loggers as pl_loggers\n","\n","# -------------------------------------------------\n","# Stuff to show the data using matplotlib\n","# -------------------------------------------------\n","#import random\n","#import numpy as np\n","#import matplotlib\n","#import matplotlib.pyplot as plt\n","# this magic command lets me show plots in the notebook\n","#%matplotlib inline\n","\n","# -------------------------------------------------\n","# Stuff for timestamping \n","# -------------------------------------------------\n","from time import process_time \n","import datetime\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fMHyUaGhj-p"},"source":["## Define the [LightningDataModule](https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html) to prepares the data for use by the network."]},{"cell_type":"markdown","metadata":{"id":"oQy1M_0Zy-eM"},"source":["A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. \n","\n","1. Download / tokenize / process.\n","1. Clean up data and (maybe) save to disk.\n","1. Load data into a [Dataset](https://pytorch.org/docs/stable/data.html).\n","1. Apply [Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) to the data (rotate, tokenize, etcâ€¦).\n","1. Wrap inside a [DataLoader](https://pytorch.org/docs/stable/data.html)."]},{"cell_type":"code","metadata":{"id":"_LTYf095fSq0"},"source":["class MyDataModule(pl.LightningDataModule):\n","\n","    def __init__(self, data_dir='./data/'):\n","        super().__init__()\n","        self.data_dir = data_dir\n","\n","        # These will be applied to every element in the dataset, to ensure they're \n","        # normalized and in the right format.\n","        self.transform = torchvision.transforms.Compose([\n","            torchvision.transforms.ToTensor(),\n","            torchvision.transforms.Normalize((0.5,), (0.5,))])\n","        \n","    # Our dataset is in the torchvision library of datasets. Here is where you'd\n","    # change the code to process a different dataset\n","    def setup(self, stage=None):\n","        self.mnist_test = datasets.MNIST(self.data_dir,train=False,download=True, transform=self.transform)\n","        mnist_full = datasets.MNIST(self.data_dir, train=True, download=True,  transform=self.transform)\n","        self.mnist_train, self.mnist_val = torch.utils.data.random_split(mnist_full, [55000, 5000])\n","\n","    # Dataloaders are the things that handle creating batches of data and handing them\n","    # to the model. You determine whether to randomize data order and the size of the batch\n","    # when you declare the data loader\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(self.mnist_train, batch_size=64, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(self.mnist_val, batch_size=64, shuffle=True)\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(self.mnist_test, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHEBDrqXAYw2"},"source":["## Define network architecture and test/train actions in a [PyTorchLightning](https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html) Module."]},{"cell_type":"markdown","metadata":{"id":"4vxVCyl317X9"},"source":["PyTorch Lightning builds on top of standard PyTorch. It is a way of organizing code to make it more modular and easier to handle. A LightningModule organizes PyTorch code into these sections:\n","\n","1. Network architecture (init)\n","1. Data-flow/computations (forward)\n","1. Train loop (training_step)\n","1. Validation loop (validation_step)\n","1. Test loop (test_step)\n","1. Optimizers (configure_optimizers)\n","\n","The first two of these (init and forward) are what you'd find in a typical [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The LightningModule extends the torch Module to add the train/test/validation and optimizer definitions into the module.\n","\n","The code you'd normally write for a torch Module's training and testing loops are instead handled externally to your code by a [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html). The Trainer handles the event loop for training and testing the model, calling the methods in your LightningModule when it's time to train or test on a batch of data from a dataloader."]},{"cell_type":"code","metadata":{"id":"p_Qr7940ORwn"},"source":["\n","class MyLightningModule(pl.LightningModule):\n","\n","  # Define the model architecture\n","  def __init__(self):\n","    super(MyLightningModule, self).__init__()\n","\n","    # mnist images are (1, 28, 28) (channels, width, height) \n","    self.layer_1 = torch.nn.Linear(28 * 28, 64)\n","    self.layer_2 = torch.nn.Linear(64, 256)\n","    self.layer_3 = torch.nn.Linear(256, 10)\n","\n","  def forward(self, x):\n","      batch_size, channels, width, height = x.size()\n","      x = x.view(batch_size, -1)\n","      x = torch.relu(self.layer_1(x))\n","      x = torch.relu(self.layer_2(x))\n","      x = torch.log_softmax(self.layer_3(x), dim=1)\n","      return x\n","\n","  def configure_optimizers(self):\n","    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","    return optimizer\n","\n","  def my_loss(self, y_hat,y):\n","      return torch.nn.functional.nll_loss(y_hat,y)\n","\n","  def training_step(self, train_batch, batch_idx):\n","      x, y = train_batch  # Here x = data, y = labels\n","      output = self.forward(x)\n","      loss = self.my_loss(output, y)\n","      \n","      # Calculate the accuracy of the model on the batch of data\n","      y_hat =  output.argmax(dim=1)\n","      accuracy = y_hat.eq(y).sum().item()/len(y)\n","\n","      # these two lines write the accurcay and loss to TensorBoard\n","      self.logger.experiment.add_scalar(\"Accuracy/Train\", accuracy)\n","      self.logger.experiment.add_scalar(\"Loss/Train\", loss)\n","\n","      return {\"loss\": loss} \n","\n","  def validation_step(self, val_batch, batch_idx):\n","      x, y = val_batch\n","      output = self.forward(x)\n","      loss = self.my_loss(output, y)\n","      self.logger.experiment.add_scalar(\"Loss/Val\", loss)\n","      return {\"loss\":loss}\n","\n","  def test_step(self, test_batch, batch_idx):\n","      x, y = test_batch\n","      output = self.forward(x)\n","      loss = self.my_loss(output, y)\n","      return {\"loss\":loss}\n","\n"," \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"je7DSC35eJk_"},"source":["## Declare a [TensorBoard](https://www.tensorflow.org/tensorboard) logger. "]},{"cell_type":"markdown","metadata":{"id":"Ke2qOKOi4l4J"},"source":["Here, we're going to set up logging to [TensorBoard](https://www.tensorflow.org/tensorboard) (the most popular way of displaying data about training your deep net), both before and after traning.  Here is a nice [tutorial on using TensorBoard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning/).\n"]},{"cell_type":"code","metadata":{"id":"M-Pzj_WudiEY"},"source":["# To clear out TensorBoard and start totally fresh, you need to\n","# remove old logs by deleting them from the directory\n","!rm -rf ./lightning_logs/\n","\n","# This will help me time-stamp my logs \n","mytime = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n","\n","# define how to log information about training to tensorboard\n","tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/','CPU',mytime)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2CCwnGievHDa"},"source":["## Declare the model and the data module"]},{"cell_type":"code","metadata":{"id":"OWKni8ePvEjo"},"source":["# load and format our data\n","data_module = MyDataModule()\n","# define our model and how it will train\n","model  = MyLightningModule()\n","\n","# saving the weights of the model for later comparison\n","untrained_model = MyLightningModule()\n","untrained_model.load_state_dict(model.state_dict())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pBHCxA2SO8l"},"source":["## Actually train the model"]},{"cell_type":"markdown","metadata":{"id":"IsJWiJyd7TLf"},"source":["PyTorch Lightning saves you from having to write a training loop because there is a [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html?highlight=Trainer) that handles calling the right DataLoader to hand a batch of training data to the Module defining the network model. The trainer is where you define the number of epochs to train for, as an example.\n","\n","Now that we've definde the network structure and dataflow in the PyTorch LigningModule, and we've defined how to load and format data in the LightningDataModule, we're ready to run the main training loop. This is where we declare the [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html?highlight=Trainer), which calls these other modules at the appropriate time.\n","\n","Note that logging to TensorBoard is happening due to some calls in the LightningModule that declared what happens in a train step and a validation step."]},{"cell_type":"code","metadata":{"id":"SAU5uu_O1PH7"},"source":["\n","# declare the traininer, which runs the training and validation loops automatically\n","trainer = pl.Trainer(logger=tb_logger, max_epochs = 2)\n","\n","# just to measure how long training takes\n","start_time = process_time()    \n","\n","# OK. Run the training loop\n","trainer.fit(model, data_module)\n","\n","print(\"Elapsed time in seconds:\",  process_time() -start_time) \n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvo2D3Q0yboM"},"source":["## Now put some more stuff on TensorBoard"]},{"cell_type":"markdown","metadata":{"id":"XjNWgL1yzWBM"},"source":["We logged training and valdation loss to TensorBoard due to the calls made in the LightningModule that defined our model. There are other things you can put on TensorBoard, including dataset images, network structure and histograms of model weights. We'll do that here."]},{"cell_type":"code","metadata":{"id":"YekjbpNTycB5"},"source":["# put a histogram of layer 1's after-training weights on TensorBoard\n","tb_logger.experiment.add_histogram('histogram of model.layer_1.weight after training', model.layer_1.weight)\n","\n","# Let's also do the before-training weights\n","tb_logger.experiment.add_histogram('histogram of model.layer_1.weight before training', untrained_model.layer_1.weight)\n","\n","# put an example batch of data on TensorBoard\n","data, target = next(iter(model.val_dataloader()))\n","show_this = torchvision.utils.make_grid(data)\n","tb_logger.experiment.add_image('validation images', show_this)\n","\n","#Putting the network structure on tensorboard\n","tb_logger.experiment.add_graph(model,data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CkHT8s9CKlTc"},"source":["## View how training went using TensorBoard. \n"]},{"cell_type":"markdown","metadata":{"id":"lhHZYYiE8foE"},"source":["Now we can start up [TensorBoard](https://www.tensorflow.org/tensorboard).TensorBoard provides  visualization and tooling for machine learning experimentation. Let's view what we've logged about training accuracy/loss, our data, and the model structure.\n","\n","Here is a good basic [tutorial on using TensorBoard with Pytorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning/)"]},{"cell_type":"code","metadata":{"id":"IqgiTM2-I1MD"},"source":["%load_ext tensorboard\n","%tensorboard --logdir lightning_logs/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLWe-SRgJluo"},"source":["## Check that we can access the GPU"]},{"cell_type":"markdown","metadata":{"id":"xpE2vpmVJse6"},"source":["Starting here, we're going to care about the GPU. Let's just check to make sure things are OK on that front. When you run the code below, you should see that CUDA is available = TRUE....if you want to do anything on the GPU. This won't affect CPU processing at all.\n","\n","Note...if you're running CoLab then, for this test to show that CUDA is available, you'll need to go to the menu bar above and select \"Runtime\", then \"Chang Runtime Type\", then select \"GPU\". Once you've done that, this test may still fail. At that point, try \"Runtime\", then \"Factory Reset Runtime\".  Once you've done that, execute this notebook again, starting from the first cell.\n"]},{"cell_type":"code","metadata":{"id":"h72byGpMHjQE"},"source":["# This will show us details about the GPU\n","!nvidia-smi\n","\n","# This will tell us the torch version, in case there's an issue there (the latest version\n","# as of this writing is 1.6)\n","print(\"My version of PyTorch is: \", torch.__version__)\n","\n","# If this turns out to be \"true\", we're probably in good shape\n","print(\"CUDA is available = \", torch.cuda.is_available())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"445nHrSmQZTU"},"source":["## Train the model again, but this time on the GPU instead of the CPU."]},{"cell_type":"markdown","metadata":{"id":"lsPGIZeROcr7"},"source":["Now...we do train a new model from scratch, but this time with the trainer set to gpus=1. That will tell it there is 1 GPU to use...and it will use that GPU. Here's the ONLY difference between this block of code and the previous block where we declared and ran a Trainer.\n","\n","Previous:\n","\n","\n","```\n","trainer = pl.Trainer(logger=tb_logger, max_epochs = 1, callbacks=[MyCallback()])\n","```\n","\n","\n","Runs on GPU:\n","\n","\n","\n","```\n","trainer = pl.Trainer(logger=tb_logger, max_epochs = 1, callbacks=[MyCallback()], gpus=[0])\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"zHaU4vSl_8ht"},"source":["# This will help me time-stamp my new log and keep is separate from the old one \n","mytime = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n","# define how to log information about training to tensorboard\n","tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/','GPU',mytime)\n","\n","# declare the traininer, which runs the training and validation loops automatically\n","trainer = pl.Trainer(logger=tb_logger, max_epochs = 2, gpus=1)\n","\n","# OK. Run the training loop\n","start_time = process_time()    \n","\n","trainer.fit(model, data_module)\n","\n","print(\"Elapsed time in seconds:\",  process_time() -start_time)  "],"execution_count":null,"outputs":[]}]}