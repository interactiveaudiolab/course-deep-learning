{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_8_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SOskmjZ5kGwz",
        "ObMMbMP4z08M",
        "r5uhOwxyV_Hg"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 8: RNNs\n",
        "\n",
        "In this notebook, we'll look at __Recurrent Neural Networks (RNNs)__ for a name/language classification task. First, we'll implement an RNN as a simple _cell_ using a for-loop. Then, we'll see how PyTorch's built-in modules allow us to deal with RNNs as conventional _layers_ in a network.\n",
        "\n",
        "This notebook borrows from several tutorials. If you want to dive into RNNs, check out:\n",
        "* [PyTorch RNNs from \"Scratch\"](https://jaketae.github.io/study/pytorch-rnn/)  \n",
        "* [PyTorch RNN Classification Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)  \n",
        "* [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)  \n",
        "\n",
        "The notebook is broken up as follows:\n",
        "\n",
        "  1. [Setup](#setup)  \n",
        "  2. [Data Preparation](#data)  \n",
        "  3. [A Simple RNN Cell](#cell)  \n",
        "  4. [A Simple RNN Layer](#layer)  "
      ],
      "metadata": {
        "id": "h6q2Ez9IPlNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __1.__ <a name=\"setup\">Setup</a>"
      ],
      "metadata": {
        "id": "SOskmjZ5kGwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper code from the course repository\n",
        "!git clone https://github.com/interactiveaudiolab/course-deep-learning.git\n",
        "# install common pacakges used for deep learning\n",
        "!cd course-deep-learning/ && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf74at7WkUnz",
        "outputId": "7cda45e7-0f9d-4368-a805-91d728269036"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'course-deep-learning'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 391 (delta 4), reused 10 (delta 3), pack-reused 379\u001b[K\n",
            "Receiving objects: 100% (391/391), 138.20 MiB | 22.79 MiB/s, done.\n",
            "Resolving deltas: 100% (183/183), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.12.0+cu113)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.4.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.11.0+cu113)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.8.0)\n",
            "Collecting ipython>=7.0\n",
            "  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (4.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.64.0)\n",
            "Collecting numpy<=1.21\n",
            "  Downloading numpy-1.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=4.4.0->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (2.1.9)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.0->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.0->-r requirements.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->-r requirements.txt (line 5)) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.0->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.0->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.46.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.3.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 9)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r requirements.txt (line 9)) (5.3.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->-r requirements.txt (line 13)) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->-r requirements.txt (line 13)) (2022.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 9)) (4.10.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 9)) (22.3.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=4.4.0->-r requirements.txt (line 3)) (1.7.1)\n",
            "Installing collected packages: numpy, prompt-toolkit, ipython\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.33.0 numpy-1.21.0 prompt-toolkit-3.0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download a dataset of names/languages \n",
        "%cd course-deep-learning/\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8yDMS5DqKH4",
        "outputId": "ca9bd81f-c192-4fb8-9126-ac76d85f4b7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/course-deep-learning\n",
            "--2022-05-22 22:40:27--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.64.174.23, 18.64.174.109, 18.64.174.119, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.64.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-22 22:40:27 (21.6 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string"
      ],
      "metadata": {
        "id": "OrQ7kpuc05Is"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __2.__ <a name=\"data\">Data Preparation</a>\n",
        "\n",
        "In this notebook, we'll train an RNN to predict the language from which a given name originates. To do so, we'll need to create a dataset mapping names to languages. We'll start by creating a dictionary in which the keys are names of languages, and the values are lists of names from the corresponding language."
      ],
      "metadata": {
        "id": "ObMMbMP4z08M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_names_dict(data_dir: str):\n",
        "  \"\"\"\n",
        "  Given a directory containing name/language data, create a dictionary mapping\n",
        "  each language to a list of string names\n",
        "  \"\"\"\n",
        "\n",
        "  # data is stored in text files, by language\n",
        "  name_files = glob.glob(f'{data_dir}/names/*.txt')\n",
        "  \n",
        "  # convert unicode strings to ASCII strings; see https://stackoverflow.com/a/518232/2809427\n",
        "  letters = string.ascii_letters + \" .,;'\"\n",
        "  n_letters = len(letters)\n",
        "\n",
        "  def unicodeToAscii(s):\n",
        "      return ''.join(\n",
        "          c for c in unicodedata.normalize('NFD', s)\n",
        "          if unicodedata.category(c) != 'Mn'\n",
        "          and c in letters\n",
        "      )\n",
        "  \n",
        "  # prepare to store data\n",
        "  dataset = {}\n",
        "\n",
        "  for name_file in name_files:\n",
        "\n",
        "    # keep track of all possible languages (labels)\n",
        "    language = os.path.splitext(os.path.basename(name_file))[0]\n",
        "    \n",
        "    # read in names for given language\n",
        "    lines = open(name_file, encoding='utf-8').read().strip().split('\\n')\n",
        "    lines = [unicodeToAscii(line) for line in lines]\n",
        "    dataset[language] = lines\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "Fx4BB3r82x47"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_dict = create_names_dict(\"data\")\n",
        "languages = list(names_dict.keys())\n",
        "\n",
        "print(f\"Languages: {languages}\")\n",
        "print(f\"Example names for {languages[0]}: {names_dict[languages[0]][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNL89SACHlwp",
        "outputId": "57e6255a-c9d7-4220-fdcb-f639dde49835"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Languages: ['Japanese', 'Irish', 'Czech', 'Spanish', 'Italian', 'Arabic', 'Portuguese', 'Vietnamese', 'German', 'Dutch', 'Chinese', 'Korean', 'Scottish', 'Greek', 'English', 'Russian', 'Polish', 'French']\n",
            "Example names for Japanese: ['Abe', 'Abukara', 'Adachi', 'Aida', 'Aihara', 'Aizawa', 'Ajibana', 'Akaike', 'Akamatsu', 'Akatsuka']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a great start, but there's one issue -- our data are strings! Just like in Notebook 3, we'll have to reformat our data as `torch.Tensor` objects in order to pass them to a neural network. How can we encode string data in a tensor format?\n",
        "\n",
        "We could represent characters as integers, and store each name as an integer-valued vector. The problem is, if we feed such a numeric representation to our network directly -- e.g., to a linear layer -- it will attempt to infer ordinal relationships between these integer encodings that don't exist!\n",
        "\n",
        "\n",
        "```\n",
        "> name = torch.as_tensor([2, 0, 17, 11])  # \"CARL\"\n",
        "...\n",
        ">> prediction = my_network(name)\n",
        "...\n",
        ">>> hidden_state = my_linear_layer(name)\n",
        "...\n",
        ">>>> output = weight_matrix @ name\n",
        "...\n",
        ">>>>> result = 10.04 * 2 -3.70 * 0 + 2.32 * 7 + 0.25 * 11\n",
        "```"
      ],
      "metadata": {
        "id": "mvKO1dDLG-Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One solution is to use __one-hot encodings__. \n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img width=\"600px\" src=\"https://drive.google.com/uc?export=view&id=109z3YkGtMnlxSHK6gxHhwub5bT3yGkuG\"/>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "\n",
        "This allows us to represent inputs numerically (so that we can pass them to a neural network), but without implying any ordinal relationship between, say, `\"C\"` and `\"A\"`."
      ],
      "metadata": {
        "id": "l8sQ9q5uKkRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_tensor(s: string):\n",
        "\n",
        "  # to be safe, cast to lower case\n",
        "  s = s.lower()\n",
        "\n",
        "  # store alphabet of possible characters\n",
        "  letters = string.ascii_letters + \" .,;'\"\n",
        "\n",
        "  # initialize tensor to hold one-hot encoding\n",
        "  t = torch.zeros(len(s), 1, len(letters))\n",
        "\n",
        "  # loop through string and encode each character\n",
        "  for i, c in enumerate(s):\n",
        "    t[i][0][letters.find(c)] = 1\n",
        "\n",
        "  return t"
      ],
      "metadata": {
        "id": "_YIMb3VCLQ3o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"CARL\"\n",
        "t = string_to_tensor(s)\n",
        "print(f\"Resulting one-hot-encoded tensor for string `{s}` has shape {t.shape}\")\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmiyUkQeM3_2",
        "outputId": "2cb3d2fa-3f03-4a56-f59e-6e3ef94c0b08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resulting one-hot-encoded tensor for string `CARL` has shape torch.Size([4, 1, 57])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've encoded our inputs numerically, we can also encode our targets (languages). This is a simple classification task -- given one input name, we want to predict one target language -- and so we can train with [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), just as we have before. Recall that this loss function expects a __prediction__ in the form of a vector of class scores, and a __ground-truth__ target in the form of an integer index. So we just need to map languages to integers."
      ],
      "metadata": {
        "id": "dImDD6tHWc8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_dataset(data_dir: str):\n",
        "  \"\"\"\n",
        "  Create a names/languages dataset in which both inputs (names) and \n",
        "  targets (languages) are encoded in tensor format.\n",
        "  \"\"\"\n",
        "\n",
        "  # create string dataset\n",
        "  names_dict = create_names_dict(data_dir)\n",
        "  languages = list(names_dict.keys())\n",
        "\n",
        "  # convert names to tensor form (one-hot)\n",
        "  names_dict = {k: [string_to_tensor(s) for s in v] for k, v in names_dict.items()}\n",
        "\n",
        "  # convert languages to tensor form (integer indices)\n",
        "  dataset = {\n",
        "      torch.tensor([languages.index(l)], dtype=torch.long) : names_dict[l] for l in languages\n",
        "  }\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "j4Zw9RstXRzq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, our network's inputs will be tensors with shape `(L, B, V)`, where\n",
        "  * `L` is the sequence length, i.e. the number of letters in the name\n",
        "  *  `B` is the batch size (for now, 1, as we will be predicting over one name at a time)\n",
        "  *  `V` is the vocabulary size, i.e. the number of possible letters (and thus the size of one-hot encoded vectors)\n",
        "\n",
        "This is a bit different from our image-domain datasets, where we dealt with inputs of shape `(B, C, H, W)` or `(B, C, H*W)`. As we'll see below, it is often convenient to put the __sequence dimension__ first when training RNNs due to the looping that occurs over this dimension. However, we'll also see how it's possible to use the more familiar __batch-first__ data format."
      ],
      "metadata": {
        "id": "YI-gBS6QdpJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __3.__ <a name=\"cell\">A Simple RNN Cell</a>\n",
        "\n",
        "It's now time to define our network. Note that our dataset consists of __variable-length__ tensors representing names (inputs) and single-element tensors representing languages (targets). Whereas a feedforward network requires fixed-length inputs, we can define an RNN model that loops over the sequence (length) dimension of inputs before finally producing a prediction. At each time step, we'll have to update the RNN's __hidden state__ using both the  input for that time step and the RNN's hidden state from the previous time step. At the final time step, we'll produce an __output state__ (prediction) in the form of a vector of class scores. This task is represented in the third image from the left below (\"many to one\"). We'll explore some of these other recurrent tasks later.\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img width=\"600px\" src=\"https://karpathy.github.io/assets/rnn/diags.jpeg\"/>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "Image source: \"The Unreasonable Effectiveness of Recurrent Neural Networks\" (Karpathy)\n",
        "</center>\n",
        "</p>\n",
        "\n",
        "\n",
        "This \"raw\" definition of an RNN, in which we handle the looping and state-update process manually, is often referred to as a __cell__. Later, we'll see how we can also use PyTorch to abstract away these details and treat RNNs as more conventional __layers__ in a network."
      ],
      "metadata": {
        "id": "r5uhOwxyV_Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "                input_size: int, \n",
        "                hidden_size: int, \n",
        "                output_size: int):\n",
        "      \n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # weight matrix 1: (inputs, previous hidden state) --> hidden state\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    # weight matrix 2: hidden state --> output state\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    \"\"\"\n",
        "    At each time step, concatenate the inputs for the current time step\n",
        "    with the hidden state from the past time step and compute the current\n",
        "    hidden state. Then, use the current hidden state to compute the current\n",
        "    output state (which we'll only use at out final time step).\n",
        "    \"\"\"\n",
        "    \n",
        "    # ensure that input represents only a single time-step\n",
        "    if input.ndim > 2:\n",
        "      L, B, V, *_ = input.shape\n",
        "      assert L == 1\n",
        "      input = input.squeeze(0)  # reduce to (B, V)\n",
        "\n",
        "    # concatenate inputs and previous hidden state\n",
        "    combined = torch.cat((input, hidden), -1)  # assume \"feature\" dimension is last\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "SGgwFZm_WDWo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train in earnest, let's see how our RNN cell processes a single input sequence."
      ],
      "metadata": {
        "id": "Dq8V8EdMgTwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 64\n",
        "n_languages = 18\n",
        "n_letters = 57\n",
        "\n",
        "rnncell = RNNCell(\n",
        "    input_size=n_letters,    # inputs are one-hot-encoded (one entry per possible letter)\n",
        "    hidden_size=hidden_size, # pick a large enough hidden size\n",
        "    output_size=n_languages  # output a vector of class scores, one per language\n",
        ")\n",
        "\n",
        "# an example input\n",
        "x = string_to_tensor(\"CARL\")\n",
        "print(f'Input shape: {x.shape}\\n')\n",
        "\n",
        "# feed input to RNN cell: loop over sequence dimension\n",
        "for i, x_i in enumerate(x):\n",
        "\n",
        "  print(f'Input shape for time step {i + 1}: {x_i.shape}')\n",
        "\n",
        "  # at first time step, RNN cell has no previous hidden state!\n",
        "  # we need to initialize one to feed in\n",
        "  if not i:\n",
        "    hidden_state = torch.zeros(1, rnncell.hidden_size)\n",
        "  \n",
        "  # pass in inputs for current time step and previous hidden state\n",
        "  output_state, hidden_state = rnncell(x_i, hidden_state)\n",
        "\n",
        "  print(f'Output state and hidden state shapes after update: {output_state.shape}, {hidden_state.shape}\\n')\n",
        "\n",
        "# our final output state will be our prediction!\n",
        "print(f'Final output state (prediction) shape: {output_state.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwdYhXTIhuBl",
        "outputId": "5107af76-0476-4fe3-ae27-22d9b0214df7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 1, 57])\n",
            "\n",
            "Input shape for time step 1: torch.Size([1, 57])\n",
            "Output state and hidden state shapes after update: torch.Size([1, 18]), torch.Size([1, 64])\n",
            "\n",
            "Input shape for time step 2: torch.Size([1, 57])\n",
            "Output state and hidden state shapes after update: torch.Size([1, 18]), torch.Size([1, 64])\n",
            "\n",
            "Input shape for time step 3: torch.Size([1, 57])\n",
            "Output state and hidden state shapes after update: torch.Size([1, 18]), torch.Size([1, 64])\n",
            "\n",
            "Input shape for time step 4: torch.Size([1, 57])\n",
            "Output state and hidden state shapes after update: torch.Size([1, 18]), torch.Size([1, 64])\n",
            "\n",
            "Final output state (prediction) shape: torch.Size([1, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything we need to train our RNN. Let's see if this thing can learn to map names to languages! We'll let our enthusiasm get the best of us and ignore validation for now."
      ],
      "metadata": {
        "id": "fWgDF41WkMUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_predict(x: torch.Tensor, rnncell: nn.Module):\n",
        "  \"\"\"Loop along sequence dimension of given input to get prediction\"\"\"\n",
        "\n",
        "  assert x.ndim == 3  # (L, B, V)\n",
        "\n",
        "  # feed input to RNN cell: loop over sequence dimension\n",
        "  for i, x_i in enumerate(x):\n",
        "\n",
        "    # at first time step, RNN cell has no previous hidden state!\n",
        "    # we need to initialize one to feed in\n",
        "    if not i:\n",
        "      hidden_state = torch.zeros(1, rnncell.hidden_size)\n",
        "  \n",
        "    # pass in inputs for current time step and previous hidden state\n",
        "    output_state, hidden_state = rnncell(x_i, hidden_state)\n",
        "\n",
        "  # return final output state\n",
        "  return output_state\n"
      ],
      "metadata": {
        "id": "sAQjWWCwvf8m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_laguages = 18\n",
        "n_letters = 57\n",
        "hidden_size = 256\n",
        "\n",
        "rnncell = RNNCell(\n",
        "    input_size=n_letters,    # inputs are one-hot-encoded (one entry per possible letter)\n",
        "    hidden_size=hidden_size, # pick a large enough hidden size\n",
        "    output_size=n_languages  # output a vector of class scores, one per language\n",
        ")\n",
        "rnncell.train()  # training mode\n",
        "\n",
        "dataset = create_final_dataset(\"data\")\n",
        "languages = list(dataset.keys())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "\n",
        "optimizer = torch.optim.SGD(rnncell.parameters(), lr=lr)\n",
        "\n",
        "max_iter = 100_000\n",
        "\n",
        "pbar = tqdm(range(max_iter), total=max_iter)\n",
        "for i in pbar:\n",
        "\n",
        "  # select a random training example: random language and random name\n",
        "  language = random.choice(languages)\n",
        "  name = random.choice(dataset[language])\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # compute prediction (scores for each language)\n",
        "  prediction = rnn_predict(name, rnncell)\n",
        "\n",
        "  # compute loss\n",
        "  loss = criterion(prediction, language)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # we'll keep our logging minimal for now -- just print loss periodically\n",
        "  if not i % 1000:\n",
        "    pbar.set_description(f'loss at iter {i}: {loss.item() :0.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dD2L5l_sDnv",
        "outputId": "ad0518ca-e78b-41a4-90bf-a8d9d0c2430c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss at iter 99000: 1.4660: 100%|██████████| 100000/100000 [03:02<00:00, 546.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out our trained networks predictions."
      ],
      "metadata": {
        "id": "H_CIP7d6PcoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnncell.eval()\n",
        "\n",
        "language_names = list(create_names_dict(\"data\").keys())\n",
        "\n",
        "name = \"Telemakos\"\n",
        "\n",
        "def get_language_name(pred: torch.Tensor):\n",
        "  return language_names[pred.argmax().item()]\n",
        "\n",
        "pred = rnn_predict(string_to_tensor(name), rnncell)\n",
        "print(f'Predicted language for name {name}: {get_language_name(pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljutgXiyzRxv",
        "outputId": "139aded3-c805-42cc-88ac-7c4e0b90c621"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language for name Telemakos: Greek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this exercise isn't really to train an accurate language predictor, but rather to get familiar with the mechanics of operating an RNN. PyTorch has built-in implementations of a variety of RNN cell types, such as LSTM. \n",
        "\n",
        "However, some of these cells behave differently. For example, LSTM expects both a hidden state _and_ a cell state at each time step. Moreover, the LSTM implementation does not include a linear layer to project the internal cell state to an output state of the required size (in our case, the number of languages). We'll have to tweak our code and handle this projection externally in our prediction function."
      ],
      "metadata": {
        "id": "amt7Fb_lRNeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.cell = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "    self.out_proj = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, state: tuple):\n",
        "\n",
        "    # unpack prior state: hidden and cell states\n",
        "    hx, cx = state\n",
        "\n",
        "    return self.cell(x, (hx, cx))  # (h_x, c_x) for next time step\n",
        "  \n",
        "  def project(self, cell_state: torch.Tensor):\n",
        "\n",
        "    return self.out_proj(cell_state)"
      ],
      "metadata": {
        "id": "yOrmNHt4U1Ls"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_predict(x: torch.Tensor, lstmcell: nn.Module):\n",
        "  \"\"\"Loop along sequence dimension of given input to get prediction\"\"\"\n",
        "\n",
        "  assert x.ndim == 3  # (L, B, V)\n",
        "\n",
        "  # feed input to RNN cell: loop over sequence dimension\n",
        "  for i, x_i in enumerate(x):\n",
        "\n",
        "    # at first time step, RNN cell has no previous hidden state!\n",
        "    # we need to initialize one to feed in\n",
        "    if not i:\n",
        "      hidden_state = torch.zeros(1, lstmcell.hidden_size)\n",
        "      cell_state = torch.zeros(1, lstmcell.hidden_size)\n",
        "  \n",
        "    # pass in inputs for current time step and previous hidden state\n",
        "    hidden_state, cell_state = lstmcell(x_i, (hidden_state, cell_state))\n",
        "\n",
        "  # project final output state\n",
        "  output_state = lstmcell.project(cell_state)\n",
        "\n",
        "  # return final output state\n",
        "  return output_state\n"
      ],
      "metadata": {
        "id": "yHW8IR2wUirQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_laguages = 18\n",
        "n_letters = 57\n",
        "hidden_size = 256\n",
        "\n",
        "lstmcell = LSTMCell(\n",
        "    input_size=n_letters,    # inputs are one-hot-encoded (one entry per possible letter)\n",
        "    hidden_size=hidden_size, # pick a large enough hidden size\n",
        "    output_size=n_languages  # output a vector of class scores, one per language\n",
        ")\n",
        "lstmcell.train()  # training mode\n",
        "\n",
        "dataset = create_final_dataset(\"data\")\n",
        "languages = list(dataset.keys())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.005\n",
        "\n",
        "optimizer = torch.optim.SGD(lstmcell.parameters(), lr=lr)\n",
        "\n",
        "max_iter = 100_000\n",
        "\n",
        "pbar = tqdm(range(max_iter), total=max_iter)\n",
        "for i in pbar:\n",
        "\n",
        "  # select a random training example: random language and random name\n",
        "  language = random.choice(languages)\n",
        "  name = random.choice(dataset[language])\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # compute prediction (scores for each language)\n",
        "  prediction = lstm_predict(name, lstmcell)\n",
        "\n",
        "  # compute loss\n",
        "  loss = criterion(prediction, language)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # we'll keep our logging minimal for now -- just print loss periodically\n",
        "  if not i % 1000:\n",
        "    pbar.set_description(f'loss at iter {i}: {loss.item() :0.4f}')"
      ],
      "metadata": {
        "id": "6quWg6eBRg6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we won't perform an in-depth comparison of different RNN cell types here. The important takeaway is that __different RNN cell types maintain different kinds of internal states, and you may have to modify your code correspondingly.__\n",
        "\n",
        "Before we move on, let's take a look at our LSTM's predictions:"
      ],
      "metadata": {
        "id": "eFN_-zuNXJ9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstmcell.eval()\n",
        "\n",
        "language_names = list(create_names_dict(\"data\").keys())\n",
        "\n",
        "name = \"Telemakos\"\n",
        "\n",
        "def get_language_name(pred: torch.Tensor):\n",
        "  return language_names[pred.argmax().item()]\n",
        "\n",
        "pred = lstm_predict(string_to_tensor(name), lstmcell)\n",
        "print(f'Predicted language for name {name}: {get_language_name(pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a_-zb5hW-UP",
        "outputId": "bc20e7bf-877f-4f92-a344-c0f75239e47e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language for name Telemakos: Greek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __4.__ <a name=\"layer\">A Simple RNN Layer</a>\n",
        "\n",
        "In the code above, we dealt with RNNs in a \"cell\" format. This required us to manually loop over the sequence dimension of our data, passing both inputs and prior states at each time step. While this level of control can be useful, we will often want to deal with RNNs in the same simplified way as other network architectures we have seen this term. That is, we want to deal with RNNs as network __layers__ to which we can hand batched data, just like multi-layer perceptrons or convolutional networks.\n",
        "\n",
        "Our first step will be to modify our data-loading code. We'll wrap our names and languages in a more conventional `Dataset` object."
      ],
      "metadata": {
        "id": "vnS0Ba3SaCU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, data_dict: dict):\n",
        "      super().__init__()\n",
        "\n",
        "      # store inputs (variable-length tensors) and targets \n",
        "      # (single-element tensors) in list format\n",
        "      self.x = []\n",
        "      self.y = []\n",
        "\n",
        "      for language, names in data_dict.items():\n",
        "\n",
        "        self.x.extend(names)\n",
        "        self.y.extend([language] * len(names))\n",
        "    \n",
        "  def __len__(self):\n",
        "    \"\"\"A required method of Dataset subclasses.\"\"\"\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"A required method of Dataset subclasses.\"\"\"\n",
        "\n",
        "    x = self.x[idx]\n",
        "    y = self.y[idx]\n",
        "\n",
        "    # create batch tensors by padding to maximum sequence length\n",
        "    if isinstance(x, list):\n",
        "      \n",
        "      # we still want to keep track of each individual name's length\n",
        "      lengths = [len(x_i) for x_i in x]\n",
        "\n",
        "      # handle padding and tensor concatenation\n",
        "      x = torch.nn.utils.rnn.pad_sequence(x)\n",
        "      \n",
        "      # reshaping inputs to batch-first format can be more legible\n",
        "      x = x.squeeze(2).permute(1, 0, 2)\n",
        "\n",
        "    else:\n",
        "\n",
        "      # we still want to keep track of each individual name's length\n",
        "      lengths = [len(x)]\n",
        "\n",
        "      # reshaping inputs to batch-first format can be more legible\n",
        "      x = x.permute(1, 0, 2)\n",
        "    \n",
        "    # we'll also put our targets in tensor format\n",
        "    if isinstance(y, list):\n",
        "      y = torch.cat(y, dim=0)    \n",
        "\n",
        "    # return resulting tensors and actual lengths\n",
        "    return x, y, lengths \n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  \"\"\"\n",
        "  Given a batch of tuples fetched from the dataset, make sure\n",
        "  input tensors are padded to maximum length. This is essentially \n",
        "  what `torch.nn.utils.rnn.pad_sequence()` does -- but for the sake\n",
        "  of learning, let's try it ourselves.\n",
        "  \"\"\"\n",
        "  x, y, lengths = zip(*batch)\n",
        "\n",
        "  _, _, v = next(iter(x)).shape\n",
        "  b = len(x)\n",
        "  max_l = max([x_i.shape[1] for x_i in x])\n",
        "\n",
        "  x_padded = torch.zeros(b, max_l, v)\n",
        "\n",
        "  for i, x_i in enumerate(x):\n",
        "    x_padded[i, :x_i.shape[1], :] = x_i\n",
        "\n",
        "  y = torch.cat(y)\n",
        "\n",
        "  lengths = [next(iter(l)) for l in lengths]\n",
        "\n",
        "  return x_padded, y, lengths\n",
        "\n",
        "\n",
        "data_dict = create_final_dataset(\"data\")\n",
        "dataset = RNNDataset(data_dict)\n",
        "\n",
        "# example batch\n",
        "x, y, lengths = dataset[:10]\n",
        "\n",
        "# inputs have shape (B, MAX_L, V) where MAX_L is largest length in batch\n",
        "print(f'Example batch of 10 instances: x (shape {x.shape}), y (shape {y.shape}), lengths {lengths}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNO249aWr9PB",
        "outputId": "726830a7-db70-4153-dc0d-e931a04aaa13"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example batch of 10 instances: x (shape torch.Size([10, 8, 57])), y (shape torch.Size([10])), lengths [3, 7, 6, 4, 6, 6, 7, 6, 8, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll define our recurrent network using the layer paradigm."
      ],
      "metadata": {
        "id": "hQ93PIZMQIvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               input_size: int, \n",
        "               hidden_size: int, \n",
        "               output_size: int,\n",
        "               num_layers: int = 1\n",
        "               ):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.rnn = nn.RNN(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    self.out_proj = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, lengths: list):\n",
        "\n",
        "    # require batched inputs: (B, MAX_L, V)\n",
        "    assert x.ndim == 3\n",
        "    b, l, v = x.shape\n",
        "\n",
        "    # built-in PyTorch layer handles loop along sequence dimension,\n",
        "    # including passing hidden state back each time step. It also \n",
        "    # handles creating a new initial state for each batch!\n",
        "    output, hidden = self.rnn(x)\n",
        "\n",
        "    # for each item in batch, take final output state (according to lengths)\n",
        "    output = torch.stack([output[i][lengths[i] - 1] for i in range(b)])\n",
        "\n",
        "    # apply final linear layer to get predictions\n",
        "    output = self.out_proj(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "trXzJjM6D2nD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll finish up by training for the same lanuage prediction task, this time using these more familiar network and dataset formats."
      ],
      "metadata": {
        "id": "3AP_uBLWbHBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize dataset\n",
        "n_letters = 57\n",
        "n_languages = 18\n",
        "data_dict = create_final_dataset(\"data\")\n",
        "dataset = RNNDataset(data_dict)\n",
        "\n",
        "# initialize data loader for random batching\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# initialize network, optimizer, and loss function\n",
        "model = RNNLayer(\n",
        "    input_size=n_letters,\n",
        "    hidden_size=256,\n",
        "    output_size=n_languages,\n",
        "    num_layers=2  # let's try 2 stacked RNN layers!\n",
        ")\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train!\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  correct = 0\n",
        "  n = 0\n",
        "\n",
        "  pbar = tqdm(enumerate(loader), total=len(loader))\n",
        "  for j, batch in pbar:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x, y, lengths = batch\n",
        "\n",
        "    predictions = model(x, lengths)\n",
        "\n",
        "    loss = criterion(predictions, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pbar.set_description(f'Loss: {loss.item() :0.4f}')\n",
        "\n",
        "    # track training accuracy per epoch\n",
        "    correct += ((predictions.argmax(dim=-1) == y) > 0).sum().item()\n",
        "    n += len(y)\n",
        "  \n",
        "  print(f'Epoch {epoch + 1} training accuracy: {correct/n :0.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn_8cSdZbvfC",
        "outputId": "f19f26e2-be89-4bd7-d11c-e6d65155f2f0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 1.219748854637146: 100%|██████████| 1004/1004 [00:18<00:00, 54.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 training accuracy: 0.5564411676795855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.5989579558372498: 100%|██████████| 1004/1004 [00:17<00:00, 56.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 training accuracy: 0.6723124439573578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.431468665599823: 100%|██████████| 1004/1004 [00:17<00:00, 56.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 training accuracy: 0.710072730895686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.7258985638618469: 100%|██████████| 1004/1004 [00:17<00:00, 56.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 training accuracy: 0.7239214904852047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.732782781124115: 100%|██████████| 1004/1004 [00:19<00:00, 52.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 training accuracy: 0.7357278071136794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.34172964096069336: 100%|██████████| 1004/1004 [00:19<00:00, 52.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 training accuracy: 0.7477333864700608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.7524484992027283: 100%|██████████| 1004/1004 [00:18<00:00, 54.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 training accuracy: 0.7562020524060974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.7793447375297546: 100%|██████████| 1004/1004 [00:18<00:00, 54.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 training accuracy: 0.7648699810700409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 1.0252188444137573: 100%|██████████| 1004/1004 [00:17<00:00, 56.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 training accuracy: 0.7708976785892199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.861352801322937: 100%|██████████| 1004/1004 [00:17<00:00, 56.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 training accuracy: 0.7781209524758393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's try out our trained RNN model. We'll tweak our code slightly to accomodate our new assumptions on the data format."
      ],
      "metadata": {
        "id": "fS84lTywJQkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "language_names = list(create_names_dict(\"data\").keys())\n",
        "\n",
        "name = \"Marcello\"\n",
        "\n",
        "def get_language_name(pred: torch.Tensor):\n",
        "  return language_names[pred.argmax(dim=-1).item()]\n",
        "\n",
        "pred = model(string_to_tensor(name).permute(1, 0, 2), [len(name)])\n",
        "\n",
        "print(f'Predicted language for name {name}: {get_language_name(pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYzocqm7IkrK",
        "outputId": "6f5231a8-aaeb-48fd-c136-471ec0bf335b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language for name Marcello: Italian\n"
          ]
        }
      ]
    }
  ]
}